{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c07f6f4-7af1-4d2d-b249-5f9e2ca524e1",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://docs.pytorch.org/tutorials/_images/cifar10.png\"\n",
    "         alt=\"data\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Cifar 10 samples</a> (Pythorch).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1169a4f8-300b-4a7d-82d1-32f61a643730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing modules ##\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# for file system\n",
    "import os\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for model evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "\n",
    "# for randimization\n",
    "import random\n",
    "\n",
    "# type\n",
    "from typing import List , Dict , Tuple\n",
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896522b-c9bb-4618-b0d0-f38adb07ce17",
   "metadata": {},
   "source": [
    "## Data Preparation and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6750517a-d46b-4430-9ca9-67e467f53623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms object to use in data augmentation\n",
    "\n",
    "mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # Randomly rotate some images by 20 degrees\n",
    "    transforms.RandomRotation(20),\n",
    "    # Randomly adjust color jitter of the images\n",
    "    transforms.ColorJitter(brightness = 0.1,contrast = 0.1,saturation = 0.1),\n",
    "    # Randomly adjust sharpness\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor = 2,p = 0.2),\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() ,\n",
    "    #randomly erase a pixel\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.RandomErasing(p=0.75,scale=(0.02, 0.1),value=1.0, inplace=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c36f8-a4ca-41ef-9955-5cbb1fb07563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training data\n",
    "\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=data_transform, # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False, # get test data\n",
    "    download=True,\n",
    "    transform=data_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d37d1-eb96-44f7-8119-51b32711f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see classes\n",
    "\n",
    "class_names = train_data.classes\n",
    "\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25583345-556b-4949-b248-9fe351dff13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Output:\n",
    "\n",
    "[‘airplane’,\n",
    "‘automobile’,\n",
    "‘bird’,\n",
    "‘cat’,\n",
    "‘deer’,\n",
    "‘dog’,\n",
    "‘frog’,\n",
    "‘horse’,\n",
    "‘ship’,\n",
    "‘truck’]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e32dee-b583-479f-afce-7f7b8c482f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into the data we've downloaded\n",
    "\n",
    "rand_idx = random.sample(range(len(train_data)),k=16)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i , idx in enumerate(rand_idx):\n",
    "\n",
    "  img , label = train_data[idx]\n",
    "\n",
    "  #the image tensor's range is not between 0 and 1,so we have to temporarily scale the tensor values into range 0 and 1 to prevent error.\n",
    "  img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "  img_class = class_names[label]\n",
    "\n",
    "  plt.subplot(4,4,i+1)\n",
    "  plt.imshow(img.permute(1,2,0))\n",
    "  plt.title(f\"Class : {img_class}\",fontsize=10)\n",
    "  plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9034de-50f9-4850-8a08-94c04c9dc7b2",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*IAOls0mcIWEMCzfDRhUsIA.png\"\n",
    "         alt=\"data\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Cifar 10 samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6735eaf-fa1e-4ce3-8df4-c98e1df06594",
   "metadata": {},
   "source": [
    "## load data into dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e521bdc-baea-47f2-abf3-bdc9bb8169cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into dataloader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 800\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=NUM_WORKERS,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d944500-4a8d-437a-96ba-945aaf7ab282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop function\n",
    "def train_step(model:nn.Module,\n",
    "               dataloader:torch.utils.data.DataLoader,\n",
    "               loss_fn:nn.Module,\n",
    "               optimizer:torch.optim.Optimizer,\n",
    "               scheduler:torch.optim.lr_scheduler = None,\n",
    "               grad_clip:float=None):\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  train_loss , train_acc = 0 , 0\n",
    "\n",
    "  for batch , (X,y) in enumerate(dataloader):\n",
    "\n",
    "    X , y = X.to(device) , y.to(device)\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    train_loss += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    if grad_clip:\n",
    "      nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "\n",
    "    optimizer.step()\n",
    "    y_pred_class = torch.argmax(y_pred,dim=1)\n",
    "    train_acc += (y_pred_class == y).sum().item() / len(y)\n",
    "\n",
    "  train_loss /= len(dataloader)\n",
    "  train_acc /= len(dataloader)\n",
    "\n",
    "  if scheduler is not None:\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "\n",
    "  return train_loss , train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59feae2-73eb-42f7-be6b-2a9333f99745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loop function\n",
    "def test_step(model:nn.Module,\n",
    "              dataloader:torch.utils.data.DataLoader,\n",
    "              loss_fn:nn.Module):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  test_loss , test_acc = 0 , 0\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    for batch , (X,y) in enumerate(dataloader):\n",
    "      X , y = X.to(device) , y.to(device)\n",
    "      test_pred_logits = model(X)\n",
    "      loss = loss_fn(test_pred_logits,y)\n",
    "      test_loss += loss.item()\n",
    "      test_pred_labels = torch.argmax(test_pred_logits,dim=1)\n",
    "      test_acc += (test_pred_labels == y).sum().item() / len(y)\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader)\n",
    "\n",
    "  return test_loss , test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f75c3-f8f1-49ac-b523-341d47f52f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a train() function to combine train_step() and test_step()\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          scheduler:torch.optim.lr_scheduler,\n",
    "          grad_clip:float=None,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 10):\n",
    "\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           scheduler=scheduler,\n",
    "                                           grad_clip=grad_clip)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52decf-7edd-426f-bf3f-2c081ba8683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot loss & accuracy curve\n",
    "\n",
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "\n",
    "\n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fed0a-bbae-46ad-963b-0403adb6c1f6",
   "metadata": {},
   "source": [
    "## Resnet 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831a669-0288-48d2-8e82-b5c06fa921c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Resnet model class\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def conv_block(self, input_channels, output_channels, use_pool=False):\n",
    "        layers = [nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1),\n",
    "                  nn.BatchNorm2d(output_channels),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "        if use_pool: layers.append(nn.MaxPool2d(2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __init__(self, input_channels, number_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = self.conv_block(input_channels, 64)\n",
    "        self.conv2 = self.conv_block(64, 128, use_pool=True)\n",
    "        self.residual1 = nn.Sequential(self.conv_block(128, 128), self.conv_block(128, 128))\n",
    "\n",
    "        self.conv3 = self.conv_block(128, 256, use_pool=True)\n",
    "        self.conv4 = self.conv_block(256, 512, use_pool=True)\n",
    "        self.residual2 = nn.Sequential(self.conv_block(512, 512), self.conv_block(512, 512))\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(512, number_classes))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        layer1 = self.conv1(xb)\n",
    "        layer2 = self.conv2(layer1)\n",
    "        residual1 = self.residual1(layer2) + layer2\n",
    "        layer3 = self.conv3(residual1)\n",
    "        layer4 = self.conv4(layer3)\n",
    "        residual2 = self.residual2(layer4) + layer4\n",
    "        class_output = self.classifier(residual2)\n",
    "        return class_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cc100-3884-43d7-b2c1-696a82bb8090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model object\n",
    "try:\n",
    "  import torchinfo\n",
    "except:\n",
    "  %pip install torchinfo\n",
    "  import torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "model_0 = ResNet9(3,10).to(device)\n",
    "\n",
    "\n",
    "#[batch_size,color_channels,width,height]\n",
    "summary(model_0, input_size=[1, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d359d86-c46e-4dcf-942e-55d4ec390842",
   "metadata": {},
   "source": [
    "==========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "==========================================================================================\n",
    "ResNet9                                  [1, 10]                   --\n",
    "├─Sequential: 1-1                        [1, 64, 32, 32]           --\n",
    "│    └─Conv2d: 2-1                       [1, 64, 32, 32]           1,792\n",
    "│    └─BatchNorm2d: 2-2                  [1, 64, 32, 32]           128\n",
    "│    └─ReLU: 2-3                         [1, 64, 32, 32]           --\n",
    "├─Sequential: 1-2                        [1, 128, 16, 16]          --\n",
    "│    └─Conv2d: 2-4                       [1, 128, 32, 32]          73,856\n",
    "│    └─BatchNorm2d: 2-5                  [1, 128, 32, 32]          256\n",
    "│    └─ReLU: 2-6                         [1, 128, 32, 32]          --\n",
    "│    └─MaxPool2d: 2-7                    [1, 128, 16, 16]          --\n",
    "├─Sequential: 1-3                        [1, 128, 16, 16]          --\n",
    "│    └─Sequential: 2-8                   [1, 128, 16, 16]          --\n",
    "│    │    └─Conv2d: 3-1                  [1, 128, 16, 16]          147,584\n",
    "│    │    └─BatchNorm2d: 3-2             [1, 128, 16, 16]          256\n",
    "│    │    └─ReLU: 3-3                    [1, 128, 16, 16]          --\n",
    "│    └─Sequential: 2-9                   [1, 128, 16, 16]          --\n",
    "│    │    └─Conv2d: 3-4                  [1, 128, 16, 16]          147,584\n",
    "│    │    └─BatchNorm2d: 3-5             [1, 128, 16, 16]          256\n",
    "│    │    └─ReLU: 3-6                    [1, 128, 16, 16]          --\n",
    "├─Sequential: 1-4                        [1, 256, 8, 8]            --\n",
    "│    └─Conv2d: 2-10                      [1, 256, 16, 16]          295,168\n",
    "│    └─BatchNorm2d: 2-11                 [1, 256, 16, 16]          512\n",
    "│    └─ReLU: 2-12                        [1, 256, 16, 16]          --\n",
    "│    └─MaxPool2d: 2-13                   [1, 256, 8, 8]            --\n",
    "├─Sequential: 1-5                        [1, 512, 4, 4]            --\n",
    "│    └─Conv2d: 2-14                      [1, 512, 8, 8]            1,180,160\n",
    "│    └─BatchNorm2d: 2-15                 [1, 512, 8, 8]            1,024\n",
    "│    └─ReLU: 2-16                        [1, 512, 8, 8]            --\n",
    "│    └─MaxPool2d: 2-17                   [1, 512, 4, 4]            --\n",
    "├─Sequential: 1-6                        [1, 512, 4, 4]            --\n",
    "│    └─Sequential: 2-18                  [1, 512, 4, 4]            --\n",
    "│    │    └─Conv2d: 3-7                  [1, 512, 4, 4]            2,359,808\n",
    "│    │    └─BatchNorm2d: 3-8             [1, 512, 4, 4]            1,024\n",
    "│    │    └─ReLU: 3-9                    [1, 512, 4, 4]            --\n",
    "│    └─Sequential: 2-19                  [1, 512, 4, 4]            --\n",
    "│    │    └─Conv2d: 3-10                 [1, 512, 4, 4]            2,359,808\n",
    "│    │    └─BatchNorm2d: 3-11            [1, 512, 4, 4]            1,024\n",
    "│    │    └─ReLU: 3-12                   [1, 512, 4, 4]            --\n",
    "├─Sequential: 1-7                        [1, 10]                   --\n",
    "│    └─MaxPool2d: 2-20                   [1, 512, 1, 1]            --\n",
    "│    └─Flatten: 2-21                     [1, 512]                  --\n",
    "│    └─Linear: 2-22                      [1, 10]                   5,130\n",
    "==========================================================================================\n",
    "Total params: 6,575,370\n",
    "Trainable params: 6,575,370\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 379.64\n",
    "==========================================================================================\n",
    "Input size (MB): 0.01\n",
    "Forward/backward pass size (MB): 6.03\n",
    "Params size (MB): 26.30\n",
    "Estimated Total Size (MB): 32.34\n",
    "=========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff93e813-0503-4248-9dcc-bd8b81ba416d",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2b31f-2855-44c5-8d15-c12e31edbf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train\n",
    "NUM_EPOCHS = 20\n",
    "learning_rate = 0.01\n",
    "weight_decay = 15e-5\n",
    "grad_clip = 0.0001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_0.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode='min',verbose=True,factor=0.3,patience=3,threshold=0.09)\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "\n",
    "model_0_results = train(model=model_0,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        grad_clip=grad_clip,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7f52811-95aa-4d59-9aee-3ac90b696dfa",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "Epoch: 1 | train_loss: 2.6319 | train_acc: 0.2716 | test_loss: 2.1258 | test_acc: 0.1856\n",
    "Epoch: 2 | train_loss: 1.4662 | train_acc: 0.4599 | test_loss: 2.0044 | test_acc: 0.2719\n",
    "Epoch: 3 | train_loss: 1.2646 | train_acc: 0.5480 | test_loss: 2.2306 | test_acc: 0.2397\n",
    "Epoch: 4 | train_loss: 1.1357 | train_acc: 0.5980 | test_loss: 1.7082 | test_acc: 0.3865\n",
    "Epoch: 5 | train_loss: 1.0942 | train_acc: 0.6178 | test_loss: 2.0035 | test_acc: 0.3102\n",
    "Epoch: 6 | train_loss: 1.0869 | train_acc: 0.6204 | test_loss: 1.5467 | test_acc: 0.4426\n",
    "Epoch: 7 | train_loss: 1.0676 | train_acc: 0.6288 | test_loss: 2.2533 | test_acc: 0.2690\n",
    "Epoch 00008: reducing learning rate of group 0 to 3.0000e-03.\n",
    "Epoch: 8 | train_loss: 1.0986 | train_acc: 0.6158 | test_loss: 1.7457 | test_acc: 0.3289\n",
    "Epoch: 9 | train_loss: 0.8463 | train_acc: 0.7054 | test_loss: 1.2220 | test_acc: 0.5703\n",
    "Epoch: 10 | train_loss: 0.7609 | train_acc: 0.7370 | test_loss: 1.2125 | test_acc: 0.5800\n",
    "Epoch: 11 | train_loss: 0.7543 | train_acc: 0.7386 | test_loss: 1.6596 | test_acc: 0.4630\n",
    "Epoch: 12 | train_loss: 0.7508 | train_acc: 0.7403 | test_loss: 1.1615 | test_acc: 0.5959\n",
    "Epoch: 13 | train_loss: 0.7364 | train_acc: 0.7452 | test_loss: 1.4978 | test_acc: 0.5075\n",
    "Epoch 00014: reducing learning rate of group 0 to 9.0000e-04.\n",
    "Epoch: 14 | train_loss: 0.7370 | train_acc: 0.7470 | test_loss: 1.0631 | test_acc: 0.6317\n",
    "Epoch: 15 | train_loss: 0.6067 | train_acc: 0.7899 | test_loss: 0.7517 | test_acc: 0.7401\n",
    "Epoch: 16 | train_loss: 0.5413 | train_acc: 0.8162 | test_loss: 0.6364 | test_acc: 0.7839\n",
    "Epoch: 17 | train_loss: 0.5228 | train_acc: 0.8197 | test_loss: 0.6756 | test_acc: 0.7678\n",
    "Epoch: 18 | train_loss: 0.5055 | train_acc: 0.8254 | test_loss: 0.8840 | test_acc: 0.7065\n",
    "Epoch: 19 | train_loss: 0.4929 | train_acc: 0.8310 | test_loss: 0.6992 | test_acc: 0.7609\n",
    "Epoch: 20 | train_loss: 0.4817 | train_acc: 0.8337 | test_loss: 0.7591 | test_acc: 0.7462\n",
    "Total training time: 1107.653 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826bcc7-e369-4533-879c-562b43762052",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(model_0_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f49ee39-e5d6-4317-86b5-c208cd79bd93",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*wA-0f8nXM6MZwYKKZaouxg.png\"\n",
    "         alt=\"data\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">courbe loss acuracy Cifar 10 samples</a> (20 epochs ).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8c82e-457b-44e9-b068-763b89604a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add more EPOCH\n",
    "NUM_EPOCHS = 30\n",
    "learning_rate = 0.001\n",
    "weight_decay = 15e-5\n",
    "grad_clip = 0.0001\n",
    "\n",
    "model_1 = ResNet9(3,10).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_1.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode='min',verbose=True,factor=0.3,patience=3,threshold=0.09)\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "\n",
    "model_1_results = train(model=model_1,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        grad_clip=grad_clip,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81df40e7-fa18-406d-9061-740d80972d3b",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "Epoch: 1 | train_loss: 1.5608 | train_acc: 0.4596 | test_loss: 1.3958 | test_acc: 0.5092\n",
    "Epoch: 2 | train_loss: 1.0262 | train_acc: 0.6352 | test_loss: 1.0752 | test_acc: 0.6192\n",
    "Epoch: 3 | train_loss: 0.8457 | train_acc: 0.7025 | test_loss: 0.9957 | test_acc: 0.6569\n",
    "Epoch: 4 | train_loss: 0.7425 | train_acc: 0.7415 | test_loss: 1.0441 | test_acc: 0.6474\n",
    "Epoch: 5 | train_loss: 0.6715 | train_acc: 0.7669 | test_loss: 0.9127 | test_acc: 0.6781\n",
    "Epoch: 6 | train_loss: 0.6221 | train_acc: 0.7841 | test_loss: 0.7769 | test_acc: 0.7297\n",
    "Epoch: 7 | train_loss: 0.5801 | train_acc: 0.8021 | test_loss: 0.7343 | test_acc: 0.7456\n",
    "Epoch: 8 | train_loss: 0.5501 | train_acc: 0.8097 | test_loss: 0.7533 | test_acc: 0.7442\n",
    "Epoch: 9 | train_loss: 0.5210 | train_acc: 0.8234 | test_loss: 0.7729 | test_acc: 0.7260\n",
    "Epoch: 10 | train_loss: 0.5018 | train_acc: 0.8285 | test_loss: 0.7651 | test_acc: 0.7385\n",
    "Epoch: 11 | train_loss: 0.4886 | train_acc: 0.8341 | test_loss: 1.1015 | test_acc: 0.6258\n",
    "Epoch: 12 | train_loss: 0.4705 | train_acc: 0.8378 | test_loss: 0.7228 | test_acc: 0.7532\n",
    "Epoch: 13 | train_loss: 0.4516 | train_acc: 0.8439 | test_loss: 0.8520 | test_acc: 0.7037\n",
    "Epoch: 14 | train_loss: 0.4411 | train_acc: 0.8501 | test_loss: 0.7881 | test_acc: 0.7301\n",
    "Epoch: 15 | train_loss: 0.4280 | train_acc: 0.8528 | test_loss: 0.8303 | test_acc: 0.7220\n",
    "Epoch: 16 | train_loss: 0.4156 | train_acc: 0.8589 | test_loss: 0.7113 | test_acc: 0.7651\n",
    "Epoch: 17 | train_loss: 0.4090 | train_acc: 0.8594 | test_loss: 0.7119 | test_acc: 0.7548\n",
    "Epoch: 18 | train_loss: 0.4034 | train_acc: 0.8648 | test_loss: 0.6782 | test_acc: 0.7711\n",
    "Epoch 00019: reducing learning rate of group 0 to 3.0000e-04.\n",
    "Epoch: 19 | train_loss: 0.3958 | train_acc: 0.8664 | test_loss: 0.6093 | test_acc: 0.7911\n",
    "Epoch: 20 | train_loss: 0.2990 | train_acc: 0.9028 | test_loss: 0.4264 | test_acc: 0.8521\n",
    "Epoch: 21 | train_loss: 0.2512 | train_acc: 0.9168 | test_loss: 0.4664 | test_acc: 0.8406\n",
    "Epoch: 22 | train_loss: 0.2356 | train_acc: 0.9235 | test_loss: 0.4288 | test_acc: 0.8509\n",
    "Epoch: 23 | train_loss: 0.2187 | train_acc: 0.9293 | test_loss: 0.4549 | test_acc: 0.8446\n",
    "Epoch: 24 | train_loss: 0.2082 | train_acc: 0.9321 | test_loss: 0.4315 | test_acc: 0.8528\n",
    "Epoch: 25 | train_loss: 0.2025 | train_acc: 0.9344 | test_loss: 0.4068 | test_acc: 0.8610\n",
    "Epoch: 26 | train_loss: 0.1959 | train_acc: 0.9369 | test_loss: 0.4843 | test_acc: 0.8362\n",
    "Epoch: 27 | train_loss: 0.1913 | train_acc: 0.9378 | test_loss: 0.4280 | test_acc: 0.8568\n",
    "Epoch: 28 | train_loss: 0.1799 | train_acc: 0.9430 | test_loss: 0.4789 | test_acc: 0.8410\n",
    "Epoch: 29 | train_loss: 0.1812 | train_acc: 0.9416 | test_loss: 0.5984 | test_acc: 0.8055\n",
    "Epoch: 30 | train_loss: 0.1710 | train_acc: 0.9449 | test_loss: 0.4142 | test_acc: 0.8601\n",
    "Total training time: 1683.720 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ffb18-c356-45a3-826d-d6d729f2b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(model_1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d4ba4-25f5-4646-98ff-5b1e728547b5",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*Etoe2PX76paz9JOgtrGWpA.png\"\n",
    "         alt=\"data\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">courbe loss acuracy Cifar 10 samples</a> (30 epochs ).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a305a-3f6e-4f38-acbe-0ac1bba76276",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Let's reduce weight_decay\n",
    "weight_decay = 35e-5\n",
    "grad_clip = 0.0001\n",
    "\n",
    "model_2 = ResNet9(3,10).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_2.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode='min',verbose=True,factor=0.3,patience=3,threshold=0.09)\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "\n",
    "model_2_results = train(model=model_2,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        grad_clip=grad_clip,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51ec8f7a-f453-4a33-ac3c-bf425a4450fc",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "Epoch: 1 | train_loss: 1.6078 | train_acc: 0.4423 | test_loss: 1.5581 | test_acc: 0.4312\n",
    "Epoch: 2 | train_loss: 1.0576 | train_acc: 0.6261 | test_loss: 1.2412 | test_acc: 0.5781\n",
    "Epoch: 3 | train_loss: 0.8773 | train_acc: 0.6943 | test_loss: 1.0260 | test_acc: 0.6265\n",
    "Epoch: 4 | train_loss: 0.7922 | train_acc: 0.7259 | test_loss: 1.1565 | test_acc: 0.6064\n",
    "Epoch: 5 | train_loss: 0.7195 | train_acc: 0.7518 | test_loss: 1.3673 | test_acc: 0.5503\n",
    "Epoch: 6 | train_loss: 0.6777 | train_acc: 0.7659 | test_loss: 0.7921 | test_acc: 0.7327\n",
    "Epoch: 7 | train_loss: 0.6525 | train_acc: 0.7776 | test_loss: 0.8481 | test_acc: 0.7084\n",
    "Epoch: 8 | train_loss: 0.6319 | train_acc: 0.7840 | test_loss: 0.9688 | test_acc: 0.6654\n",
    "Epoch: 9 | train_loss: 0.6082 | train_acc: 0.7929 | test_loss: 0.8870 | test_acc: 0.6890\n",
    "Epoch: 10 | train_loss: 0.6024 | train_acc: 0.7954 | test_loss: 0.9684 | test_acc: 0.6539\n",
    "Epoch 00011: reducing learning rate of group 0 to 3.0000e-04.\n",
    "Epoch: 11 | train_loss: 0.5976 | train_acc: 0.7968 | test_loss: 0.7982 | test_acc: 0.7248\n",
    "Epoch: 12 | train_loss: 0.4628 | train_acc: 0.8467 | test_loss: 0.6369 | test_acc: 0.7791\n",
    "Epoch: 13 | train_loss: 0.4080 | train_acc: 0.8628 | test_loss: 0.5645 | test_acc: 0.8073\n",
    "Epoch: 14 | train_loss: 0.3871 | train_acc: 0.8702 | test_loss: 0.5591 | test_acc: 0.8066\n",
    "Epoch: 15 | train_loss: 0.3699 | train_acc: 0.8770 | test_loss: 0.5647 | test_acc: 0.8027\n",
    "Epoch: 16 | train_loss: 0.3575 | train_acc: 0.8810 | test_loss: 0.5046 | test_acc: 0.8312\n",
    "Epoch: 17 | train_loss: 0.3487 | train_acc: 0.8849 | test_loss: 0.5715 | test_acc: 0.7993\n",
    "Epoch: 18 | train_loss: 0.3408 | train_acc: 0.8882 | test_loss: 0.4850 | test_acc: 0.8376\n",
    "Epoch: 19 | train_loss: 0.3310 | train_acc: 0.8904 | test_loss: 0.6566 | test_acc: 0.7859\n",
    "Epoch: 20 | train_loss: 0.3268 | train_acc: 0.8917 | test_loss: 0.5754 | test_acc: 0.7979\n",
    "Epoch: 21 | train_loss: 0.3261 | train_acc: 0.8936 | test_loss: 0.5239 | test_acc: 0.8239\n",
    "Epoch: 22 | train_loss: 0.3240 | train_acc: 0.8934 | test_loss: 0.6099 | test_acc: 0.7938\n",
    "Epoch 00023: reducing learning rate of group 0 to 9.0000e-05.\n",
    "Epoch: 23 | train_loss: 0.3204 | train_acc: 0.8959 | test_loss: 0.6680 | test_acc: 0.7770\n",
    "Epoch: 24 | train_loss: 0.2501 | train_acc: 0.9197 | test_loss: 0.4134 | test_acc: 0.8586\n",
    "Epoch: 25 | train_loss: 0.2156 | train_acc: 0.9324 | test_loss: 0.3898 | test_acc: 0.8701\n",
    "Epoch: 26 | train_loss: 0.2069 | train_acc: 0.9359 | test_loss: 0.3907 | test_acc: 0.8662\n",
    "Epoch: 27 | train_loss: 0.1955 | train_acc: 0.9401 | test_loss: 0.3950 | test_acc: 0.8669\n",
    "Epoch: 28 | train_loss: 0.1831 | train_acc: 0.9437 | test_loss: 0.4053 | test_acc: 0.8618\n",
    "Epoch: 29 | train_loss: 0.1823 | train_acc: 0.9437 | test_loss: 0.4601 | test_acc: 0.8444\n",
    "Epoch: 30 | train_loss: 0.1715 | train_acc: 0.9490 | test_loss: 0.3909 | test_acc: 0.8669\n",
    "Total training time: 1614.319 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5796359-eedb-4899-a7a8-eb967cd4a7c0",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c536ca-977c-49ce-bac7-fb4352a461d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more training iterations\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "\n",
    "model_2_results_exten = train(model=model_2,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        grad_clip=grad_clip,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc72130e-779b-45cd-bf35-582071f02204",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "Epoch: 1 | train_loss: 0.1674 | train_acc: 0.9498 | test_loss: 0.3767 | test_acc: 0.8717\n",
    "Epoch: 2 | train_loss: 0.1620 | train_acc: 0.9510 | test_loss: 0.3921 | test_acc: 0.8667\n",
    "Epoch: 3 | train_loss: 0.1586 | train_acc: 0.9524 | test_loss: 0.4013 | test_acc: 0.8669\n",
    "Epoch 00034: reducing learning rate of group 0 to 2.7000e-05.\n",
    "Epoch: 4 | train_loss: 0.1579 | train_acc: 0.9528 | test_loss: 0.3930 | test_acc: 0.8647\n",
    "Epoch: 5 | train_loss: 0.1275 | train_acc: 0.9642 | test_loss: 0.3153 | test_acc: 0.8941\n",
    "Epoch: 6 | train_loss: 0.1190 | train_acc: 0.9672 | test_loss: 0.3337 | test_acc: 0.8887\n",
    "Epoch: 7 | train_loss: 0.1145 | train_acc: 0.9677 | test_loss: 0.3128 | test_acc: 0.8939\n",
    "Epoch: 8 | train_loss: 0.1079 | train_acc: 0.9718 | test_loss: 0.3169 | test_acc: 0.8887\n",
    "Epoch: 9 | train_loss: 0.1047 | train_acc: 0.9723 | test_loss: 0.3203 | test_acc: 0.8960\n",
    "Epoch: 10 | train_loss: 0.1028 | train_acc: 0.9733 | test_loss: 0.3335 | test_acc: 0.8881\n",
    "Epoch: 11 | train_loss: 0.0985 | train_acc: 0.9742 | test_loss: 0.3167 | test_acc: 0.8921\n",
    "Epoch: 12 | train_loss: 0.0971 | train_acc: 0.9750 | test_loss: 0.3387 | test_acc: 0.8849\n",
    "Epoch: 13 | train_loss: 0.0950 | train_acc: 0.9760 | test_loss: 0.3269 | test_acc: 0.8891\n",
    "Epoch: 14 | train_loss: 0.0923 | train_acc: 0.9767 | test_loss: 0.3184 | test_acc: 0.8929\n",
    "Epoch: 15 | train_loss: 0.0901 | train_acc: 0.9775 | test_loss: 0.3211 | test_acc: 0.8933\n",
    "Epoch: 16 | train_loss: 0.0881 | train_acc: 0.9775 | test_loss: 0.3317 | test_acc: 0.8864\n",
    "Epoch: 17 | train_loss: 0.0858 | train_acc: 0.9781 | test_loss: 0.3277 | test_acc: 0.8905\n",
    "Epoch 00048: reducing learning rate of group 0 to 8.1000e-06.\n",
    "Epoch: 18 | train_loss: 0.0861 | train_acc: 0.9778 | test_loss: 0.3217 | test_acc: 0.8899\n",
    "Epoch: 19 | train_loss: 0.0770 | train_acc: 0.9816 | test_loss: 0.3058 | test_acc: 0.8989\n",
    "Epoch: 20 | train_loss: 0.0730 | train_acc: 0.9827 | test_loss: 0.3121 | test_acc: 0.8963\n",
    "Epoch: 21 | train_loss: 0.0708 | train_acc: 0.9842 | test_loss: 0.2971 | test_acc: 0.8992\n",
    "Epoch: 22 | train_loss: 0.0720 | train_acc: 0.9826 | test_loss: 0.3006 | test_acc: 0.8994\n",
    "Epoch 00053: reducing learning rate of group 0 to 2.4300e-06.\n",
    "Epoch: 23 | train_loss: 0.0719 | train_acc: 0.9832 | test_loss: 0.3024 | test_acc: 0.8971\n",
    "Epoch: 24 | train_loss: 0.0690 | train_acc: 0.9840 | test_loss: 0.3065 | test_acc: 0.8974\n",
    "Epoch: 25 | train_loss: 0.0669 | train_acc: 0.9848 | test_loss: 0.2973 | test_acc: 0.8987\n",
    "Epoch: 26 | train_loss: 0.0665 | train_acc: 0.9847 | test_loss: 0.3093 | test_acc: 0.8969\n",
    "Epoch: 27 | train_loss: 0.0674 | train_acc: 0.9846 | test_loss: 0.2978 | test_acc: 0.8989\n",
    "Epoch 00058: reducing learning rate of group 0 to 7.2900e-07.\n",
    "Epoch: 28 | train_loss: 0.0664 | train_acc: 0.9846 | test_loss: 0.3016 | test_acc: 0.9012\n",
    "Epoch: 29 | train_loss: 0.0646 | train_acc: 0.9854 | test_loss: 0.2985 | test_acc: 0.9015\n",
    "Epoch: 30 | train_loss: 0.0651 | train_acc: 0.9852 | test_loss: 0.2944 | test_acc: 0.8993\n",
    "Total training time: 1602.459 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6f965-d6a4-4f2f-af0e-68d68dd70589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 2 results\n",
    "\n",
    "for key in model_2_results.keys():\n",
    "  model_2_results[key].extend(model_2_results_exten[key])\n",
    "\n",
    "plot_loss_curves(model_2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c51a9-0239-4f1a-837d-4fa7cc3407a7",
   "metadata": {},
   "source": [
    " <table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*mKuvnmNXs5lvF2YOjZujvw.png\"\n",
    "         alt=\"data\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">courbe loss acuracy Cifar 10 samples</a> (60 epochs ).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ceb7f-c2dc-4e22-96b2-c6e17c04da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try predicting images with our model\n",
    "\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "\n",
    "model_2.eval()\n",
    "\n",
    "with torch.no_grad():  # We are using no_grad instead of inference_mode for better compatibility\n",
    "  for images, labels in DataLoader(dataset=test_data, batch_size=1, num_workers=NUM_WORKERS):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    prediction_logits = model_2(images)\n",
    "    predictions = prediction_logits.argmax(dim=1).cpu().numpy()\n",
    "    predicted_labels.extend(predictions)\n",
    "    true_labels = labels.cpu().numpy()\n",
    "    actual_labels.extend(true_labels)\n",
    "\n",
    "confusion_mat = confusion_matrix(actual_labels, predicted_labels)\n",
    "confusion_df = pd.DataFrame(confusion_mat/np.sum(confusion_mat)*10, index=class_names, columns=class_names)\n",
    "plt.figure(figsize=(12,7))\n",
    "sn.heatmap(confusion_df, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42572f4c-c789-476d-bc52-c585ba7b8793",
   "metadata": {},
   "source": [
    " <table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*J5vcWERSwB7MqpOSQmAIzQ.png\"\n",
    "         alt=\"data\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\"> Matrice de confusion  Cifar 10 samples</a> .<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4facb197-ec32-4ef4-8b3f-5f59fe45e5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
